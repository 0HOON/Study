# Retrieval

**Suppose you have an embedding model. Given a user, ow would you decide which items to recommend?**

- For a Matrix Factorization model, the user(query) embedding is known. The system can simply look it up from the user embedding matrix.
- For a DNN model, the system will to compute the user(query) embedding $\psi(x)$ at serve time based on the given features $x$.

Once you have the query embedding, you can decide nearest neighbors by similarity scores $s(q, V_j$) between the query embedding $q$ and item embeddings $V_j$.

In related-item recommendations, you can apply the same approach. First look up the embedding of the item, and then compare it with other item embeddings to find top-k nearest neighbors.

### Large-scale Retrieval

To find the nearest neighbors, the system need to compute similarity score of all possible candidates. This exhaustive scoring can be very expensive for large corpora. The following strategies can be helpful.

- If the query embedding is know statically, perform exhaustive scoring offline (before online service). This is a common practive for related-item recommendation
- Use approximate nearest neighbors

# Scoring

After candidate generation, **another model scores and ranks the candidates** to display. The recommendation system may have multiple candidate generators that use different features like:

- Embeddings of a MF model
- Personal user features
- Geographic informations
- Popular or trending items
- A social graph (relationship among people like friends)

The candidates generated by various generators are combined into a **common pool of candidates.** Then, a scoring model scores and ranks the candidates in the pool using several features.

- query features (like user watch history, language, county, time)
- video features (like title, tags, embedding)

## Why Not Let the Candidate Generators Score?

Actually the candidate generator scores candidates and gives top-k high-scored candidates. But you should avoid using them as score&ranking model for the following reasons:

- If you use multiple candidate generators, their scores may not be comparable.
- With a smaller pool of candidates, the system can afford to use more time to score, which means you can train and use a more complex model that takes bigger number of features into account and usually performs better.

## Choosing an Objective Function for Scoring

The choice of scoring funciton can dramatically affect the ranking of items and ultimately the quality of the recommendations. For example:

- **Maximizing Click Rate** may make the system prefer click-bait videos. It may generate clicks but does not make such a good user experience.
- **Maximizing Watch Time** may make the system prefer long videos. Obviously, multiple short wathes can be just as good as one long watch.

## Positional Bias in Scoring

The position of the items usually affects the click rates. Items that appear higher on the screen attract more people than ones appearing lower on the screen. However, scoring model usually don't know where the items would be placed. Querying the model with all possible positions is too expensive. Even if it is possible, the system still might not be able to find a consistent ranking across multiple ranking scores. The following strategy may be helpful:

- Creating position-independent rankings
- Ranki all the candidates as if they are in the same position on the screen

# Re-ranking

In the fianl stage, the system can re-rank the candidates to consider additional criteria or constraints.

## Freshness

Incorporating the latest informations such as current user history and the newest items and keeping the model fresh helps the model to offer better recommendations.

### Solutions

- **Re-run training** as often as possible with latest data. Warm-starting the training can significantly reduce the training time.
- For MF model, create an **average user** to represent new users. You can create clusters of users based on user features.
- Use **DNN model** that takes feature vector as input. It can handle new data that was not seen during training.
- Add **document age** as a feature.

## Diversity

The "closest" candidates tend to be very similar to each other. This lack of diversity can cause boring user experience.

### Solutions

- Train **multiple candidate generators** which use different sources.
- Train **multiple score & ranking model** using different objective functions
- Use **metadata of items** such as genre as features to ensure diversity.

## Fairness

Your model should treat all users fairly. Therefore, make sure your model isnâ€™t learning unconscious biases from the training data.

### Solutions

- Include **diverses perspectives** in design and development of the system.
- Use **comprehensive datasets**. Add auxiliary data if your data is too sparse or biased.
- **Track metrics** on each demographic to prevent biases.
- Make **seperate modelf for underserved groups**.
